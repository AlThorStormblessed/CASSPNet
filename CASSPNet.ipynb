{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9Fdka9RBjWP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.layers import Input, MultiHeadAttention, GlobalAveragePooling1D, LayerNormalization\n",
        "\n",
        "os.chdir(\"drive/MyDrive/Cyber_paper\") #to get into DAKOTA folder, comment out if already in there\n",
        "current_dir = os.getcwd()\n",
        "data_dir = current_dir + '/DATA'\n",
        "print(\"Current Data Dir: \", data_dir)\n",
        "all_sess = glob.glob(\"DATA/*\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHPAQ2RMuL3B"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EF18swX8Cbin"
      },
      "outputs": [],
      "source": [
        "n = 1\n",
        "sessions = all_sess[:]\n",
        "tl = sessions[n]\n",
        "del sessions[n]\n",
        "df = pd.read_csv(glob.glob(f\"{tl}/merged_features_all.csv\")[0])\n",
        "df[\"LABEL\"] = 1\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lRJ7pNQubOS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "figure, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 8))\n",
        "\n",
        "axes[0].plot(df[df.columns[0]], linewidth=0.5, zorder=1, label=df.columns[0])\n",
        "axes[0].plot(df[df.columns[8]], linewidth=0.5, zorder=1, label=df.columns[8])\n",
        "axes[0].plot(df[df.columns[16]], linewidth=0.5, zorder=1, label=df.columns[16])\n",
        "axes[0].legend()\n",
        "axes[0].set_title('Accelerometer')\n",
        "\n",
        "axes[1].plot(df[df.columns[24]], linewidth=0.5, zorder=1, label=df.columns[24])\n",
        "axes[1].plot(df[df.columns[32]], linewidth=0.5, zorder=1, label=df.columns[32])\n",
        "axes[1].plot(df[df.columns[40]], linewidth=0.5, zorder=1, label=df.columns[40])\n",
        "axes[1].legend()\n",
        "axes[1].set_title('Magnetometer')\n",
        "\n",
        "axes[2].plot(df[df.columns[48]], linewidth=0.5, zorder=1, label=df.columns[48])\n",
        "axes[2].plot(df[df.columns[56]], linewidth=0.5, zorder=1, label=df.columns[56])\n",
        "axes[2].plot(df[df.columns[64]], linewidth=0.5, zorder=1, label=df.columns[64])\n",
        "axes[2].legend()\n",
        "axes[2].set_title('Gyroscope')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6BdaQqeja1z"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Egatn5BUhqjN"
      },
      "outputs": [],
      "source": [
        "\n",
        "df2 = pd.concat((pd.read_csv(glob.glob(f\"{tl}/merged_features_all.csv\")[0]) for tl in sessions))\n",
        "df2[\"LABEL\"] = 0\n",
        "df2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqr5c-k8hqWQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "merged_df = pd.concat((df, df2))\n",
        "\n",
        "columns = merged_df.columns\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "model = scaler.fit(merged_df)\n",
        "scaled_data = pd.DataFrame(model.transform(merged_df), columns= columns)\n",
        "print(scaled_data.shape)\n",
        "scaled_data[\"LABEL\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XfNwveMnSR4"
      },
      "outputs": [],
      "source": [
        "scaled_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6reHxw29hqIq"
      },
      "outputs": [],
      "source": [
        "\n",
        "# print(scaled_data)\n",
        "\n",
        "train, validate, test = \\\n",
        "              np.split(scaled_data.sample(frac=1, random_state=42),\n",
        "                       [int(.88*len(scaled_data)), int(.9*len(scaled_data))])\n",
        "\n",
        "X_train_, y_train_ = train.drop(\"LABEL\", axis=1), train[\"LABEL\"]\n",
        "X_test, y_test = test.drop(\"LABEL\", axis=1), test[\"LABEL\"]\n",
        "X_val, y_val = validate.drop(\"LABEL\", axis=1), validate[\"LABEL\"]\n",
        "\n",
        "# print(X_train.shape, y_train.shape)\n",
        "# print(X_test.shape, y_test.shape)\n",
        "# print(X_val.shape, y_val.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "sm = ADASYN(sampling_strategy = 0.7, n_neighbors = 6)\n",
        "# print(len(X_train))\n",
        "X_train, y_train = sm.fit_resample(X_train_, y_train_)\n",
        "print(X_train.shape, y_train.shape)"
      ],
      "metadata": {
        "id": "s0Tw4IVnq9fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SVMSMOTE\n",
        "sm = SVMSMOTE(sampling_strategy = .7, k_neighbors = 10)\n",
        "# print(len(X_train))\n",
        "X_train, y_train = sm.fit_resample(X_train_, y_train_)\n",
        "print(X_train.shape, y_train.shape)"
      ],
      "metadata": {
        "id": "7-htpFn0XROV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwTiEep9kPS5"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE(sampling_strategy = 0.7, k_neighbors = 5)\n",
        "# print(len(X_train))\n",
        "X_train, y_train = sm.fit_resample(X_train_, y_train_)\n",
        "print(X_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6O3a9KpJk-qS"
      },
      "outputs": [],
      "source": [
        "y_train.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNUU64BCk0eF"
      },
      "outputs": [],
      "source": [
        "X_train = np.resize(X_train, (X_train.shape[0], 126, 1))\n",
        "X_test = np.resize(X_test, (X_test.shape[0], 126, 1))\n",
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LrPhc86EmKr"
      },
      "outputs": [],
      "source": [
        "!pip install joblib==1.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gHw0cZ4uD5g"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_len, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.positional_encoding = self.get_positional_encoding(max_len, d_model)\n",
        "\n",
        "    def get_positional_encoding(self, max_len, d_model):\n",
        "        pos = np.arange(max_len)[:, np.newaxis]\n",
        "        i = np.arange(d_model)[np.newaxis, :]\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "        angle_rads = pos * angle_rates\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = angle_rads[np.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs + self.positional_encoding[:, :tf.shape(inputs)[1], :]\n",
        "\n",
        "\n",
        "class ContAuth_trans:\n",
        "    def __init__(self, hidden_units, dropout_rate, input_shape, num_classes, learning_rate, batch_size, early_stopping_threshold, alpha):\n",
        "        self.hidden_units = hidden_units\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.early_stopping_threshold = early_stopping_threshold\n",
        "        self.alpha = alpha\n",
        "        self.model = self.create_model()\n",
        "\n",
        "    def create_model(self):\n",
        "        inputs = Input(shape=self.input_shape)\n",
        "\n",
        "        position_encoding = PositionalEncoding(self.input_shape[0], self.hidden_units)(inputs)\n",
        "\n",
        "        x = Dense(self.hidden_units, activation='relu')(position_encoding)\n",
        "        x = Dropout(self.dropout_rate)(x)\n",
        "\n",
        "        attention = MultiHeadAttention(num_heads=4, key_dim=self.hidden_units)(x, x)\n",
        "        attention = Dropout(self.dropout_rate)(attention)\n",
        "        attention = LayerNormalization()(attention)\n",
        "\n",
        "        attention = MultiHeadAttention(num_heads=8, key_dim=int(self.hidden_units/2))(attention, attention)\n",
        "        attention = Dropout(self.dropout_rate)(attention)\n",
        "        attention = LayerNormalization()(attention)\n",
        "\n",
        "        x = GlobalAveragePooling1D()(attention)\n",
        "\n",
        "        x = Dense(128, activation='relu')(x)\n",
        "        x = Dropout(self.dropout_rate)(x)\n",
        "        outputs = Dense(self.num_classes, activation='softmax')(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        return model\n",
        "\n",
        "    def compile_model(self):\n",
        "        optimizer = Adam(learning_rate=self.learning_rate)\n",
        "        self.model.compile(optimizer=optimizer,\n",
        "                           loss='sparse_categorical_crossentropy',\n",
        "                           metrics=['accuracy'])\n",
        "\n",
        "    def train_model(self, X_train, y_train, X_val, y_val, epochs):\n",
        "        early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                                       patience=20,\n",
        "                                       min_delta=self.early_stopping_threshold,\n",
        "                                       restore_best_weights=True)\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
        "                                      patience=2, min_lr=0.00001, mode='auto', verbose=1)\n",
        "        checkpointer = ModelCheckpoint(\"saved_model/model_t2.keras\", verbose=1, save_best_only=True)\n",
        "\n",
        "        history = self.model.fit(X_train, y_train,\n",
        "                                 validation_data=(X_val, y_val),\n",
        "                                 epochs=epochs,\n",
        "                                 batch_size=self.batch_size,\n",
        "                                 callbacks=[early_stopping, reduce_lr, checkpointer])\n",
        "        return history\n",
        "\n",
        "    def extract_features(self, X):\n",
        "        feature_extractor = Model(inputs=self.model.input, outputs=self.model.get_layer(index=-4).output)\n",
        "        features = feature_extractor.predict(X)\n",
        "        return features\n",
        "\n",
        "    def train_sgd_classifier(self, X_train_features, y_train, X_val_features, y_val):\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_features)\n",
        "        X_val_scaled = scaler.transform(X_val_features)\n",
        "\n",
        "        sgd = SGDClassifier(loss='hinge', penalty='elasticnet', alpha=self.alpha, l1_ratio=0.5, max_iter=1000, tol=1e-3)\n",
        "        sgd.fit(X_train_scaled, y_train)\n",
        "\n",
        "        y_val_pred = sgd.predict(X_val_scaled)\n",
        "        accuracy = accuracy_score(y_val, y_val_pred)\n",
        "        return sgd, accuracy\n",
        "\n",
        "    def data_incremental_training(self, X_new, y_new, X_test, y_test, epochs):\n",
        "        history = self.train_model(X_new, y_new, X_test, y_test, epochs)\n",
        "\n",
        "        X_train_features = self.extract_features(X_new)\n",
        "        X_test_features = self.extract_features(X_test)\n",
        "\n",
        "        sgd, accuracy = self.train_sgd_classifier(X_train_features, y_new, X_test_features, y_test)\n",
        "\n",
        "        return history, accuracy\n",
        "\n",
        "hidden_units = 256\n",
        "dropout_rate = 0.5\n",
        "input_shape = (126, 1)\n",
        "num_classes = 2\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "early_stopping_threshold = 0.00001\n",
        "alpha = 0.01\n",
        "\n",
        "try:\n",
        "    del model\n",
        "except:\n",
        "    pass\n",
        "\n",
        "model = ContAuth_trans(hidden_units, dropout_rate, input_shape, num_classes, learning_rate, batch_size, early_stopping_threshold, alpha)\n",
        "model.compile_model()\n",
        "\n",
        "# Print model summary\n",
        "print(model.model.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmY-vrp8B0w0"
      },
      "outputs": [],
      "source": [
        "class ContAuth:\n",
        "    def __init__(self, hidden_units, dropout_rate, input_shape, num_classes, learning_rate, batch_size, early_stopping_threshold, alpha):\n",
        "        self.hidden_units = hidden_units\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.early_stopping_threshold = early_stopping_threshold\n",
        "        self.alpha = alpha\n",
        "        self.model = self.create_model()\n",
        "\n",
        "    def create_model(self):\n",
        "        inputs = Input(shape=self.input_shape)\n",
        "        x = LSTM(self.hidden_units, return_sequences=False)(inputs)\n",
        "        x = Dropout(self.dropout_rate)(x)\n",
        "        x = Dense(int(self.hidden_units/2), activation='relu')(x)\n",
        "        x = Dropout(self.dropout_rate)(x)\n",
        "        outputs = Dense(self.num_classes, activation='sigmoid')(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        return model\n",
        "\n",
        "    def compile_model(self):\n",
        "        optimizer = Adam(learning_rate=self.learning_rate)\n",
        "        self.model.compile(optimizer=optimizer,\n",
        "                           loss='sparse_categorical_crossentropy',\n",
        "                           metrics=['accuracy'])\n",
        "\n",
        "    def train_model(self, X_train, y_train, X_val, y_val, epochs):\n",
        "        if(epochs > 20): pat = 20\n",
        "        else: pat = 2\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                                       patience=pat,\n",
        "                                       min_delta=self.early_stopping_threshold,\n",
        "                                       restore_best_weights=True)\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
        "                              patience=2, min_lr=0.00001, mode='auto', verbose=1)\n",
        "        checkpointer = ModelCheckpoint(\"saved_model/model_t2.keras\", verbose=1, save_best_only=True)\n",
        "\n",
        "        history = self.model.fit(X_train, y_train,\n",
        "                                 validation_data=(X_val, y_val),\n",
        "                                 epochs=epochs,\n",
        "                                 batch_size=self.batch_size,\n",
        "                                 callbacks=[early_stopping, reduce_lr, checkpointer])\n",
        "\n",
        "        return history\n",
        "\n",
        "    def extract_features(self, X):\n",
        "        feature_extractor = Model(inputs=self.model.input, outputs=self.model.layers[-1].output)\n",
        "        features = feature_extractor.predict(X)\n",
        "        return features\n",
        "\n",
        "    def train_sgd_classifier(self, X_train_features, y_train, X_val_features, y_val):\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_features)\n",
        "        X_val_scaled = scaler.transform(X_val_features)\n",
        "\n",
        "        sgd = SGDClassifier(loss='hinge', penalty='elasticnet', alpha=self.alpha, l1_ratio=0.5, max_iter=1000, tol=1e-3)\n",
        "        sgd.fit(X_train_scaled, y_train)\n",
        "\n",
        "        y_val_pred = sgd.predict(X_val_scaled)\n",
        "        accuracy = accuracy_score(y_val, y_val_pred)\n",
        "        return sgd, accuracy\n",
        "\n",
        "    def data_incremental_training(self, X_new, y_new, X_test, y_test, epochs):\n",
        "        X_train_features = self.extract_features(X_new)\n",
        "        X_test_features = self.extract_features(X_test)\n",
        "\n",
        "        sgd, accuracy = self.train_sgd_classifier(X_train_features, y_new, X_test_features, y_test)\n",
        "\n",
        "        y_new = y_new.values.reshape(-1, 1)\n",
        "        y_test = y_test.values.reshape(-1, 1)\n",
        "\n",
        "        history = self.train_model(X_new, y_new, X_test, y_test, epochs)\n",
        "        return history, accuracy\n",
        "\n",
        "hidden_units = 256\n",
        "dropout_rate = 0.7\n",
        "input_shape = (126, 1)\n",
        "num_classes = 2\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "early_stopping_threshold = 0.00001\n",
        "alpha = 0.01\n",
        "\n",
        "# Initialize and compile model\n",
        "try: del model\n",
        "except: pass\n",
        "\n",
        "model = ContAuth(hidden_units, dropout_rate, input_shape, num_classes, learning_rate, batch_size, early_stopping_threshold, alpha)\n",
        "model.compile_model()\n",
        "\n",
        "# Print model summary\n",
        "print(model.model.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eqwo_gLcwld8",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install mlrose-hiive\n",
        "\n",
        "import mlrose_hiive as mlrose\n",
        "import numpy as np\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "class ContAuth:\n",
        "    def __init__(self, hidden_units, dropout_rate, input_shape, num_classes, learning_rate, batch_size, early_stopping_threshold, alpha):\n",
        "        self.hidden_units = hidden_units\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.early_stopping_threshold = early_stopping_threshold\n",
        "        self.alpha = alpha\n",
        "        self.model = self.create_model()\n",
        "\n",
        "    def create_model(self):\n",
        "        inputs = Input(shape=self.input_shape)\n",
        "        x = LSTM(self.hidden_units, return_sequences=False)(inputs)\n",
        "        x = Dropout(self.dropout_rate)(x)\n",
        "        x = Dense(self.hidden_units, activation='relu')(x)\n",
        "        x = Dropout(self.dropout_rate)(x)\n",
        "        outputs = Dense(self.num_classes, activation='sigmoid')(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        return model\n",
        "\n",
        "    def compile_model(self):\n",
        "        optimizer = Adam(learning_rate=self.learning_rate)\n",
        "        self.model.compile(optimizer=optimizer,\n",
        "                           loss='sparse_categorical_crossentropy',\n",
        "                           metrics=['accuracy'])\n",
        "\n",
        "    def train_model(self, X_train, y_train, X_val, y_val, epochs):\n",
        "        if epochs > 20:\n",
        "            pat = 20\n",
        "        else:\n",
        "            pat = 2\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                                       patience=pat,\n",
        "                                       min_delta=self.early_stopping_threshold,\n",
        "                                       restore_best_weights=True)\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
        "                                      patience=2, min_lr=0.00001, mode='auto', verbose=1)\n",
        "        checkpointer = ModelCheckpoint(\"saved_model/model_t2.keras\", verbose=1, save_best_only=True)\n",
        "\n",
        "        history = self.model.fit(X_train, y_train,\n",
        "                                 validation_data=(X_val, y_val),\n",
        "                                 epochs=epochs,\n",
        "                                 batch_size=self.batch_size,\n",
        "                                 callbacks=[early_stopping, reduce_lr, checkpointer])\n",
        "\n",
        "        return history\n",
        "\n",
        "    def extract_features(self, X):\n",
        "        feature_extractor = Model(inputs=self.model.input, outputs=self.model.layers[-1].output)\n",
        "        features = feature_extractor.predict(X)\n",
        "        return features\n",
        "\n",
        "    def train_sgd_classifier(self, X_train_features, y_train, X_val_features, y_val):\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_features)\n",
        "        X_val_scaled = scaler.transform(X_val_features)\n",
        "\n",
        "        sgd = SGDClassifier(loss='hinge', penalty='elasticnet', alpha=self.alpha, l1_ratio=0.5, max_iter=1000, tol=1e-3)\n",
        "        sgd.fit(X_train_scaled, y_train)\n",
        "\n",
        "        y_val_pred = sgd.predict(X_val_scaled)\n",
        "        accuracy = accuracy_score(y_val, y_val_pred)\n",
        "        return sgd, accuracy\n",
        "\n",
        "    def data_incremental_training(self, X_new, y_new, X_test, y_test, epochs):\n",
        "        X_train_features = self.extract_features(X_new)\n",
        "        X_test_features = self.extract_features(X_test)\n",
        "\n",
        "        sgd, accuracy = self.train_sgd_classifier(X_train_features, y_new, X_test_features, y_test)\n",
        "\n",
        "        y_new = y_new.values.reshape(-1, 1)\n",
        "        y_test = y_test.values.reshape(-1, 1)\n",
        "\n",
        "        history = self.train_model(X_new, y_new, X_test, y_test, epochs)\n",
        "        return history, accuracy\n",
        "\n",
        "\n",
        "def fitness_function(params):\n",
        "    # Map params to the real hyperparameter ranges\n",
        "    hidden_units = int(params[0] * (512 - 64) + 64)  # Map 0.0-1.0 to 64-512\n",
        "    dropout_rate = params[1] * (0.7 - 0.2) + 0.2  # Map 0.0-1.0 to 0.2-0.7\n",
        "    learning_rate = params[2] * (0.01 - 0.0001) + 0.0001  # Map 0.0-1.0 to 0.0001-0.01\n",
        "    batch_size = int(params[3] * (128 - 16) + 16)  # Map 0.0-1.0 to 16-128\n",
        "\n",
        "    cont_auth_model = ContAuth(hidden_units=hidden_units,\n",
        "                               dropout_rate=dropout_rate,\n",
        "                               input_shape=(X_train.shape[1], X_train.shape[2]),\n",
        "                               num_classes=2,\n",
        "                               learning_rate=learning_rate,\n",
        "                               batch_size=batch_size,\n",
        "                               early_stopping_threshold=0.001,\n",
        "                               alpha=0.01)\n",
        "\n",
        "    cont_auth_model.compile_model()\n",
        "\n",
        "    history = cont_auth_model.train_model(X_train, y_train, X_val, y_val, epochs=5)\n",
        "\n",
        "    val_accuracy = max(history.history['val_accuracy'])\n",
        "\n",
        "    return -val_accuracy\n",
        "\n",
        "\n",
        "problem = mlrose.ContinuousOpt(length=4,  # 4 hyperparameters to optimize\n",
        "                               fitness_fn=mlrose.CustomFitness(fitness_function),\n",
        "                               maximize=True,\n",
        "                               min_val=0.0,  # Minimum value for all parameters\n",
        "                               max_val=1.0,  # Maximum value for all parameters\n",
        "                               step=0.25)\n",
        "\n",
        "best_params, best_fitness = mlrose.genetic_alg(problem, max_iters=5, random_state=42)\n",
        "\n",
        "best_hidden_units = int(best_params[0])\n",
        "best_dropout_rate = best_params[1]\n",
        "best_learning_rate = best_params[2]\n",
        "best_batch_size = int(best_params[3])\n",
        "\n",
        "print(f\"Best Hidden Units: {best_hidden_units}\")\n",
        "print(f\"Best Dropout Rate: {best_dropout_rate}\")\n",
        "print(f\"Best Learning Rate: {best_learning_rate}\")\n",
        "print(f\"Best Batch Size: {best_batch_size}\")\n",
        "\n",
        "final_model = ContAuth(hidden_units=best_hidden_units,\n",
        "                       dropout_rate=best_dropout_rate,\n",
        "                       input_shape=(X_train.shape[1], X_train.shape[2]),\n",
        "                       num_classes=2,\n",
        "                       learning_rate=best_learning_rate,\n",
        "                       batch_size=best_batch_size,\n",
        "                       early_stopping_threshold=0.001,\n",
        "                       alpha=0.01)\n",
        "\n",
        "final_model.compile_model()\n",
        "\n",
        "final_history = final_model.train_model(X_train, y_train, X_test, y_test, epochs=50)\n",
        "\n",
        "final_loss, final_accuracy = final_model.model.evaluate(X_test, y_test)\n",
        "print(f\"Final Test Accuracy: {final_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4003xGXH1_Bd"
      },
      "outputs": [],
      "source": [
        "#Initial training\n",
        "# After running one of above three\n",
        "\n",
        "history = model.train_model(X_train, y_train, X_test, y_test, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkSBlbFahh8C"
      },
      "outputs": [],
      "source": [
        "#Data incremental learning\n",
        "\n",
        "#New data is X_val, y_val\n",
        "\n",
        "X_val_train, X_val_test, y_val_train, y_val_test = train_test_split(X_val, y_val, test_size=0.2, random_state=42)\n",
        "history1, accuracy = model.data_incremental_training(X_val_train, y_val_train, X_val_test, y_val_test, 10)\n",
        "print(\"Accuracy: \", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7i_aKuo20WJO"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_loaded = model.model\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve\n",
        "predictions = model_loaded.predict(X_test)\n",
        "\n",
        "# print(len(predictions))\n",
        "# print(predictions)\n",
        "\n",
        "for sigmoid_threshold in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
        "  labels = [0 if pred > sigmoid_threshold else 1 for pred in predictions[:, 0]]\n",
        "\n",
        "  conf_matrix = confusion_matrix(y_test, labels)\n",
        "\n",
        "  TN, FP, FN, TP = conf_matrix.ravel()\n",
        "\n",
        "  FAR = FP / (FP + TN)\n",
        "  FRR = FN / (FN + TP)\n",
        "\n",
        "  print(f\"Sigmoid Threshold: {sigmoid_threshold}\")\n",
        "  print(f\"False Acceptance Rate (FAR): {FAR}\")\n",
        "  print(f\"False Rejection Rate (FRR): {FRR}\")\n",
        "\n",
        "  fpr, tpr, thresholds = roc_curve(y_test, predictions[:, 1])\n",
        "\n",
        "  frr = 1 - tpr\n",
        "\n",
        "  eer_threshold = thresholds[np.nanargmin(np.abs(frr - fpr))]\n",
        "  eer = fpr[np.nanargmin(np.abs(frr - fpr))]\n",
        "\n",
        "  # print(f\"EER Threshold: {eer_threshold}\")\n",
        "  print(f\"Equal Error Rate (EER): {eer}\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vhfxqkp1ypF3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(\"Loss_LSTM.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}